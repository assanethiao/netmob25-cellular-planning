# @package _global_
# Example configuration for VAE-LSTM starting from pretrained weights
# Usage: python scripts/train.py +experiment=vae_lstm_from_pretrained training=adaptive_training devices=[6] accelerator=gpu
defaults:
  - _self_

# Model configuration (should match the pretrained model architecture)
model:
  hidden_dim: 128
  latent_dim: 16
  num_layers: 2
  condition_dim: 32
  dropout: 0.1

# Training configuration - complete config to avoid parameter conflicts
training:
  # Path to pretrained checkpoint
  pretrained_checkpoint: experiments/vae_lstm_2025-09-06_19-32-02/checkpoints/last.ckpt
  
  # Basic training parameters
  batch_size: 256
  epochs: 20000
  learning_rate: 0.0001
  val_split: 0.1
  gradient_clip: null
  
  # Early stopping and LR scheduling
  early_stopping_enabled: false
  lr_scheduler_enabled: false
  best_metric_monitor: val_loss
  
  # Complete loss configuration with adaptive slow annealing
  loss:
    type: simple_vae
    params:
      beta:
        type: adaptive_slow_annealing
        params:
          initial_beta: 0.2400
          target_beta: 1
          beta_increment: 0.05  # Much smaller increments (20x smaller)
          patience_epochs: 50  # More patience before increasing
          improvement_threshold: 1e-5  # Smaller threshold for more sensitivity
      free_bits:
        enabled: false
        lambda_free_bits: 2.0

# Device configuration
accelerator: gpu
devices: [0]  # Adjust based on your system

seed: 42